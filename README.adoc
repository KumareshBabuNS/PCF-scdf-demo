= PCF-scdf-demo

Based on the original repo created by Adam Zwickey https://github.com/azwickey-pivotal/scdf-raven

== Prepare Env

. Download PCFDev, a full, local install of Cloudfoundry here: https://network.pivotal.io/products/pcfdev

. Install PCFDev following the documentation instructions here: http://docs.pivotal.io/pcf-dev/index.html

. Log into PCFDev using CF CLI
+
[source,bash]
---------------------------------------------------------------------
$ cf login -a api.local.pcfdev.io --skip-ssl-validation -u admin -p admin
---------------------------------------------------------------------

. Create the required RabbitMQ, MySQL and Redis service instances
+
[source,bash]
---------------------------------------------------------------------
$ cf create-service p-rabbitmq standard rabbit
$ cf create-service p-mysql 512mb mysql
$ cf create-service p-redis shared-vm redis
---------------------------------------------------------------------

== Deploy Spring Cloud Dataflow Server

. Change directories to the _scdf-server_ directory

. Deploy the Spring Cloud Dataflow server to Cloudfoundry.
(Note that there is no need to build the scdf-server as the manifest is using a pre-built jar that ihas been conveniently place in the root folder of the scdf-server)
+
[source,bash]
---------------------------------------------------------------------
$ cf push
---------------------------------------------------------------------

. After the application starts up you will have dataflow running.  If you'd like, you cann access the Dataflow UI here: http://scdf-server.local.pcfdev.io/dashboard

. By Default you will not have any Stream applications available.  We'll use the Spring Cloud Dataflow shell to load them.

. Change directories to the _scdf-shell_ directory and run the SCDF shell:
+
[source,bash]
---------------------------------------------------------------------
$ mvn clean spring-boot:run
---------------------------------------------------------------------

. After the Shell loads we need to target our deployed dataflow server:
+
[source,bash]
---------------------------------------------------------------------
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

1.0.0.M3

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".


server-unknown:> dataflow config server http://scdf-server.local.pcfdev.io
Successfully targeted http://scdf-server.local.pcfdev.io

---------------------------------------------------------------------

.  Your shell should now be targeted correctly.  Test this by listing the current streams (which should be empty right now):
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream list
╔═══════════╤═════════════════╤══════╗
║Stream Name│Stream Definition│Status║
╚═══════════╧═════════════════╧══════╝

---------------------------------------------------------------------

. Load the Spring Cloud Dataflow apps by executing this command.  Be sure to replace $LOCAL_FILESYSTEM_PATH with your correct system path to this git project:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app import --uri file://$LOCAL_FILESYSTEM_PATH/df-shell/app.properties --local TRUE

Successfully registered applications: [source.tcp, task.timestamp, source.http, sink.jdbc, sink.rabbit, source.rabbit, source.ftp, sink.gpfdist, processor.transform, source.sftp, processor.filter, source.file, sink.cassandra, processor.groovy-filter, sink.router, source.trigger, processor.splitter, sink.redis, source.load-generator, sink.file, source.time, source.twitterstream, sink.tcp, source.jdbc, sink.field-value-counter, sink.hdfs, processor.bridge, processor.pmml, processor.httpclient, sink.ftp, sink.log, sink.gemfire, sink.aggregate-counter, sink.throughput, source.jms, processor.scriptable-transform, sink.counter, sink.websocket, processor.groovy-transform]

---------------------------------------------------------------------

. List the loaded applications using the shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app list
╔══════════════╤════════════════════╤═══════════════════╤═════════╗
║    source    │     processor      │       sink        │  task   ║
╠══════════════╪════════════════════╪═══════════════════╪═════════╣
║file          │bridge              │aggregate-counter  │timestamp║
║ftp           │filter              │cassandra          │         ║
║http          │groovy-filter       │counter            │         ║
║jdbc          │groovy-transform    │field-value-counter│         ║
║jms           │httpclient          │file               │         ║
║load-generator│pmml                │ftp                │         ║
║rabbit        │scriptable-transform│gemfire            │         ║
║sftp          │splitter            │gpfdist            │         ║
║tcp           │transform           │hdfs               │         ║
║time          │                    │jdbc               │         ║
║trigger       │                    │log                │         ║
║twitterstream │                    │rabbit             │         ║
║              │                    │redis              │         ║
║              │                    │router             │         ║
║              │                    │tcp                │         ║
║              │                    │throughput         │         ║
║              │                    │websocket          │         ║
╚══════════════╧════════════════════╧═══════════════════╧═════════╝

---------------------------------------------------------------------

== Load Custom Applications and Create a Stream using the Spring Cloud Dataflow Shell

. First we'll load our custom apps that are found in the _/scdf-source_, _/scdf-sink_ and  _/scdf-mysqlsink_ directories.  The code is already compiled and Spring Cloud Dataflow will download these from GIT over HTTP.  Execute the command in the Spring Cloud Dataflow shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app import --uri file://$LOCAL_FILESYSTEM_PATH/scdf-shell/app_custom.properties --local TRUE
Successfully registered applications: [sink.sink-sample, source.source-sample, sink.sink-mysqlsample]
---------------------------------------------------------------------

. Create your first stream using two of these apps.  Execute the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream create --name test --definition "source-sample | sink-sample" --deploy
Created and deployed new stream 'test'

---------------------------------------------------------------------

. In a separate terminal window check the status of the stream deployment using the CF CLI.  The app names will be prefixed with _dataflow-test_, which is the name of your stream:
+
[source,bash]
---------------------------------------------------------------------
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                             requested state   instances   memory   disk   urls
scdf-server                      started           1/1         1G       512M   scdf-server.local.pcfdev.io
dataflow-test-sink-sample        started           1/1         1G       1G     dataflow-test-sink-sample.local.pcfdev.io
dataflow-test-source-sample      started           1/1         1G       1G     dataflow-test-source-sample.local.pcfdev.io

---------------------------------------------------------------------
. You can also access the CF DataFlow Server Dashboard to create, deploy and manage all streams and apps from the browser:
+
http://scdf-server.local.pcfdev.io/dashboard

. Tail the logs of the test-sink-sample CF application:
+
[source,bash]
---------------------------------------------------------------------
$ $ cf logs dataflow-test-sink-sample
  Connected, tailing logs for app dataflow-test-sink-sample in org pcfdev-org / space pcfdev-space as admin...

---------------------------------------------------------------------

. The _dataflow-test-source-sample_ application is listening at an /event endpoint.  Hit this endpoint using curl:
+
[source,bash]
---------------------------------------------------------------------
$ curl -k https://dataflow-test-source-sample.local.pcfdev.io/event\?msg\=I%20just%20created%20a%20data%20stream
event[I just created a data stream] placed on streaming bus%
---------------------------------------------------------------------

. Check the logs of the dataflow-test-sink-sample application (they should already be tailing in one of your windows).  You'll see the message you just posted:
+
[source,bash]
---------------------------------------------------------------------
$ cf logs dataflow-test-sink-sample
Connected, tailing logs for app dataflow-test-sink-sample in org pcfdev-org / space pcfdev-space as admin...

2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : I just created a data stream
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_receivedRoutingKey=test.source-sample
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_receivedExchange=test.source-sample
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	Header1=Sent from data microservice
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_deliveryTag=4
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_consumerQueue=test.source-sample.test
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_redelivered=false
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	id=ef984117-a9e9-bdcf-5810-5be8afc0bb7d
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_consumerTag=amq.ctag-_RxrssJUrMq6LsDvGAztnQ
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	contentType=text/plain
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	timestamp=1467293287941

---------------------------------------------------------------------

. Undeploy the firt stream. This is an optional step, but if you are running this demo in PCFDev, you won't have enough resources to have both streams up and running. Execute the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream undeploy --name test
Un-deployed stream 'test'

---------------------------------------------------------------------

== Create the new Stream that stores the messages in a MySQL database

. Undeploy the firt stream. This is an optional step, but if you are running this demo in PCFDev, you won't have enough resources to have both streams up and running. Execute the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream undeploy --name test
Un-deployed stream 'test'

---------------------------------------------------------------------

. We already loaded the all our custom applicatins earlier in this demo. Now we are going to use the  _/scdf-source_ and _/scdf-mysqlsink_ directories:

. Create the second stream executing the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream create --name testmysql --definition "source-sample | sink-mysqlsample" --deploy
Created and deployed new stream 'testmysql'

---------------------------------------------------------------------

. In a separate terminal window check the status of the stream deployment using the CF CLI.  This time the app names will be prefixed with _dataflow-testmysql_, which is the name of your stream:
+
[source,bash]
---------------------------------------------------------------------
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                             requested state   instances   memory   disk   urls
scdf-server                      started           1/1         1G       512M   scdf-server.local.pcfdev.io
dataflow-testmysql-sink-sample   started           1/1         1G       1G     dataflow-testmysql-sink-sample.local.pcfdev.io
dataflow-testmysql-source-sample started           1/1         1G       1G     dataflow-testmysql-source-sample.local.pcfdev.io

---------------------------------------------------------------------

. Tail the logs of the test-sink-sample CF application:
+
[source,bash]
---------------------------------------------------------------------
$ $ cf logs dataflow-test-sink-sample
  Connected, tailing logs for app dataflow-test-sink-sample in org pcfdev-org / space pcfdev-space as admin...

---------------------------------------------------------------------

. As with the previous stream, we will create a message hitting the endpoint of the Source application:
+
[source,bash]
---------------------------------------------------------------------
$ curl -k https://dataflow-test-source-sample.local.pcfdev.io/event\?msg\=I%20just%20created%20a%20data%20stream%20to%20write%20in%20a%20database
event[I just created a data stream to write in a database] placed on streaming bus
---------------------------------------------------------------------
